[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Roadmap to full automation in sorting in the second-hand industry\n\n\n\nAI\n\nautomation\n\nfashion\n\nsustainability\n\n\n\nAn exploration of the path to automation in the second-hand fashion industry, examining the steps needed for successful AI integration from initial assessment to full…\n\n\n\nFarrukh Nauman\n\n\nApr 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nZero-Shot Image Classification with CLIP Models\n\n\n\ndeep-learning\n\ncomputer-vision\n\nnatural-language-processing\n\n\n\nExplore how CLIP and SigLIP models enable zero-shot image classification and their unique advantages over traditional image classifiers.\n\n\n\nFarrukh Nauman\n\n\nMar 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nExploring the Second-Hand Fashion Dataset\n\n\n\ndata\n\nfashion\n\nsustainability\n\nmachine-learning\n\ncomputer-vision\n\n\n\nDeep dive into a 30,000+ image dataset for second-hand clothing classification.\n\n\n\nFarrukh Nauman\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nFuture directions\n\n\n\nAI\n\nfashion\n\nsustainability\n\n\n\nInsights into data quality challenges, organizational hurdles, and future opportunities in AI for second-hand fashion, including specialized applications in recycling and…\n\n\n\nFarrukh Nauman\n\n\nJan 30, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-02-06-data-exploration/index.html",
    "href": "posts/2025-02-06-data-exploration/index.html",
    "title": "Exploring the Second-Hand Fashion Dataset",
    "section": "",
    "text": "In this post, we’ll explore our one of a kind dataset that is focused on second-hand clothing classification."
  },
  {
    "objectID": "posts/2025-02-06-data-exploration/index.html#introduction",
    "href": "posts/2025-02-06-data-exploration/index.html#introduction",
    "title": "Exploring the Second-Hand Fashion Dataset",
    "section": "",
    "text": "In this post, we’ll explore our one of a kind dataset that is focused on second-hand clothing classification."
  },
  {
    "objectID": "posts/2025-02-06-data-exploration/index.html#dataset-overview",
    "href": "posts/2025-02-06-data-exploration/index.html#dataset-overview",
    "title": "Exploring the Second-Hand Fashion Dataset",
    "section": "Dataset Overview",
    "text": "Dataset Overview\nThe dataset contains 31,638 clothing items and was created as part of the Vinnova funded project “AI for resource-efficient circular fashion”. It’s a collaboration between RISE Research Institutes of Sweden AB, Wargön Innovation AB, and Myrorna AB, with additional support from the EU project CISUTAC.\nThe main purpose of this dataset is to help classify garments into various categories to determine their future use:\n\nReuse\nExport (reuse outside Sweden)\nRecycling\nRepair\nRemake\nThermal waste"
  },
  {
    "objectID": "posts/2025-02-06-data-exploration/index.html#interactive-dataset-explorer",
    "href": "posts/2025-02-06-data-exploration/index.html#interactive-dataset-explorer",
    "title": "Exploring the Second-Hand Fashion Dataset",
    "section": "Interactive Dataset Explorer",
    "text": "Interactive Dataset Explorer\nThe following is a version of the dataset that only uses the front imageYou can explore the dataset directly through the Hugging Face dataset viewer:\n\n\nYou can try simple search queries like “Shirt” or use the DuckDB interactive SQL editor to explore the dataset: SELECT * FROM train WHERE type = 'Shirt' AND condition = 4; for example will return all “Shirt” items that have a condition of 4 (i.e. “very good”). You might need to login to your huggingface account to use SQL queries. In my experience, the SQL queries run slowly."
  },
  {
    "objectID": "posts/2025-02-06-data-exploration/index.html#loading-the-dataset",
    "href": "posts/2025-02-06-data-exploration/index.html#loading-the-dataset",
    "title": "Exploring the Second-Hand Fashion Dataset",
    "section": "Loading the Dataset",
    "text": "Loading the Dataset\nTo work with this dataset programmatically, you can use the Hugging Face datasets library. Here’s how to load and explore the data:\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"fnauman/fashion-second-hand-front-only-rgb\")\n\n# Access the training split\ntrain_dataset = dataset[\"train\"]\n\n# Print basic information\nprint(f\"Dataset size: {len(train_dataset)} images\") # 28248\nprint(f\"Features: {train_dataset.features}\") # 19\n\n# Access an example\nexample = train_dataset[0]\n\nimage = example[\"image\"]\n# # Display the image - notebook\n# from IPython.display import display\n# display(example[\"image\"])\n\nprint(f\"Brand: {example['brand']}, Category: {example['category']}\")\n# Output: Brand: Soc (stadium), Category: Ladies\nThe image is:"
  },
  {
    "objectID": "posts/2025-02-06-data-exploration/index.html#dataset-features",
    "href": "posts/2025-02-06-data-exploration/index.html#dataset-features",
    "title": "Exploring the Second-Hand Fashion Dataset",
    "section": "Dataset Features",
    "text": "Dataset Features\nThe dataset provides RGB images of clothing items taken from the front view. Each image is labeled with 18 distinct attributes that can be used for:\n\nTraining computer vision models for clothing classification: type, category, etc.\nDetecting condition and damage.\nBuilding image-based and text-based search systems."
  },
  {
    "objectID": "posts/2025-02-06-data-exploration/index.html#license-and-attribution",
    "href": "posts/2025-02-06-data-exploration/index.html#license-and-attribution",
    "title": "Exploring the Second-Hand Fashion Dataset",
    "section": "License and Attribution",
    "text": "License and Attribution\nThe dataset is released under CC-BY 4.0 license. When using this dataset, make sure to properly attribute the zenodo version:\n\nNauman, F. (2024). Clothing Dataset for Second-Hand Fashion (Version 3) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.13788681"
  },
  {
    "objectID": "posts/2025-02-06-data-exploration/index.html#next-steps",
    "href": "posts/2025-02-06-data-exploration/index.html#next-steps",
    "title": "Exploring the Second-Hand Fashion Dataset",
    "section": "Next Steps",
    "text": "Next Steps\nIn future posts, we’ll explore:\n\nBuilding a classification model using this dataset\nCreating a simple web application for clothing classification\n\nStay tuned for more insights into sustainable fashion and AI!"
  },
  {
    "objectID": "posts/2025-03-22-zero-shot-image-classification/index.html",
    "href": "posts/2025-03-22-zero-shot-image-classification/index.html",
    "title": "Zero-Shot Image Classification with CLIP Models",
    "section": "",
    "text": "Imagine you’re sorting through a huge pile of clothes for your online second-hand store. You’ve got everything from vintage dresses to modern t-shirts, and you need to quickly categorize them all. Traditional image classifiers are like robots trained to only recognize a specific set of clothing types they’ve seen before. If you suddenly have a “bohemian skirt” – something outside their training – they’d be stumped!\nBut what if you could use a smarter kind of AI? One that can understand the idea of a “bohemian skirt” just from the words, even if it’s never seen one exactly like it before? That’s the magic of zero-shot image classification, and it’s changing how we can use computers to understand images, especially in fields like fashion where trends and categories are always evolving.\nIn this post, we’ll explore how models called CLIP (Contrastive Language-Image Pre-training) and SigLIP (Sigmoid Loss Pre-training) make this “zero-shot” magic possible. They are vision-language models, meaning they understand both images and text, allowing them to classify images in incredibly flexible ways, without needing to be specifically trained on every single category beforehand. This is a game-changer for anyone dealing with visual categorization tasks, especially in dynamic domains like fashion."
  },
  {
    "objectID": "posts/2025-03-22-zero-shot-image-classification/index.html#introduction",
    "href": "posts/2025-03-22-zero-shot-image-classification/index.html#introduction",
    "title": "Zero-Shot Image Classification with CLIP Models",
    "section": "",
    "text": "Imagine you’re sorting through a huge pile of clothes for your online second-hand store. You’ve got everything from vintage dresses to modern t-shirts, and you need to quickly categorize them all. Traditional image classifiers are like robots trained to only recognize a specific set of clothing types they’ve seen before. If you suddenly have a “bohemian skirt” – something outside their training – they’d be stumped!\nBut what if you could use a smarter kind of AI? One that can understand the idea of a “bohemian skirt” just from the words, even if it’s never seen one exactly like it before? That’s the magic of zero-shot image classification, and it’s changing how we can use computers to understand images, especially in fields like fashion where trends and categories are always evolving.\nIn this post, we’ll explore how models called CLIP (Contrastive Language-Image Pre-training) and SigLIP (Sigmoid Loss Pre-training) make this “zero-shot” magic possible. They are vision-language models, meaning they understand both images and text, allowing them to classify images in incredibly flexible ways, without needing to be specifically trained on every single category beforehand. This is a game-changer for anyone dealing with visual categorization tasks, especially in dynamic domains like fashion."
  },
  {
    "objectID": "posts/2025-03-22-zero-shot-image-classification/index.html#what-makes-clip-and-siglip-special",
    "href": "posts/2025-03-22-zero-shot-image-classification/index.html#what-makes-clip-and-siglip-special",
    "title": "Zero-Shot Image Classification with CLIP Models",
    "section": "What Makes CLIP and SigLIP Special?",
    "text": "What Makes CLIP and SigLIP Special?\nCLIP and SigLIP are fundamentally different from traditional image classifiers like ResNet or EfficientNet (and even Vision Transformers (ViT) used solely for classification) in several important ways:\n\nJoint Vision-Language Training (Contrastive Learning): This is the core difference.\n\nTraditional Classifiers (ResNet, ViT): These models are trained on images labeled with a fixed set of categories. They learn to map image features to these specific labels. They only learn visual features. The output layer has a fixed number of neurons, one for each class.\nCLIP/SigLIP: These models are trained on image-text pairs. The training objective is contrastive. Given a batch of images and texts, the model learns to:\n\nMaximize the similarity between the embeddings of an image and its correct text description.\nMinimize the similarity between the embeddings of an image and incorrect text descriptions. This is often done using a contrastive loss function (e.g., InfoNCE). SigLIP uses a sigmoid cross-entropy loss instead of the softmax-based contrastive loss in CLIP, which has been shown to improve performance. This creates a joint embedding space where images and text representing similar concepts are close together, even if the model hasn’t seen that exact combination during training.\n\n\nMassive Datasets: These models are pre-trained on 100M-1B+ image-text pairs collected from the internet. This “web” scale pretraining data ensures that the models have seen a wide range of visual and textual concepts, making them more robust and versatile.\nNo Predefined Classes (During Inference):\n\nTraditional Classifiers: Require a predefined set of output classes. You can’t ask them to classify something outside of that set without retraining or fine-tuning.\nCLIP/SigLIP: Can classify images against any arbitrary text description at inference time. The model compares the image embedding to the embeddings of the provided text descriptions. This flexibility is the essence of zero-shot learning. One potential limitation is that certain unique concepts or names like specific new brands or products may not be recognized."
  },
  {
    "objectID": "posts/2025-03-22-zero-shot-image-classification/index.html#how-zero-shot-classification-works",
    "href": "posts/2025-03-22-zero-shot-image-classification/index.html#how-zero-shot-classification-works",
    "title": "Zero-Shot Image Classification with CLIP Models",
    "section": "How Zero-Shot Classification Works",
    "text": "How Zero-Shot Classification Works\nThe core mechanism behind zero-shot classification with CLIP-like models is elegantly simple:\n\nEncoding: The model uses separate encoders (typically Transformers) for images and text. The image is passed through the image encoder, and each potential text label is passed through the text encoder. This produces an image embedding and a set of text embeddings, all within the same shared embedding space.\nSimilarity Calculation: The model then computes the similarity between the image embedding and each text embedding. Cosine similarity is commonly used.\nPrediction: The text label with the highest similarity score is the predicted class.\n\nThis approach allows you to classify images using any text descriptions you provide on the fly, without retraining the model."
  },
  {
    "objectID": "posts/2025-03-22-zero-shot-image-classification/index.html#zero-shot-classification-with-transformers",
    "href": "posts/2025-03-22-zero-shot-image-classification/index.html#zero-shot-classification-with-transformers",
    "title": "Zero-Shot Image Classification with CLIP Models",
    "section": "Zero-Shot Classification with Transformers",
    "text": "Zero-Shot Classification with Transformers\nLet’s implement a simple zero-shot image classifier using the Hugging Face transformers library:\n\n\nCode\nfrom transformers import pipeline\nfrom PIL import Image\n\n# Load a pre-trained CLIP/SigLIP model\nclassifier = pipeline(\n    \"zero-shot-image-classification\",\n    model=\"google/siglip-base-patch16-224\",\n)\n\n# Load an image\nimage = Image.open(\"fashion_item.jpg\")\n\n# Define classification labels\ncandidate_labels = [\n    \"t-shirt\", \n    \"dress\", \n    \"jacket\", \n    \"jeans\"\n]\n\n# Perform zero-shot classification\nresults = classifier(\n    image, \n    candidate_labels=candidate_labels\n)\n\n# Print results\nfor result in results:\n    print(f\"{result['label']}: {result['score']:.2%}\")\n\n# Output:\n# t-shirt: 76.32%\n# dress: 15.87%\n# jacket: 4.51%\n# jeans: 2.15%\n\n\nThe model assigns a confidence score to each label without ever being explicitly trained on fashion categories."
  },
  {
    "objectID": "posts/2025-03-22-zero-shot-image-classification/index.html#advanced-example-custom-model-selection",
    "href": "posts/2025-03-22-zero-shot-image-classification/index.html#advanced-example-custom-model-selection",
    "title": "Zero-Shot Image Classification with CLIP Models",
    "section": "Advanced Example: Custom Model Selection",
    "text": "Advanced Example: Custom Model Selection\nYou can choose from several state-of-the-art vision-language models for zero-shot classification. Here’s a more advanced example with model selection:\n\n\nCode\nimport torch\nfrom transformers import pipeline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Available models\nMODELS = {\n    \"siglip-base-patch16-224\": \"google/siglip-base-patch16-224\", \n    \"siglip2-base-patch16-224\": \"google/siglip2-base-patch16-224\",\n    \"siglip2-so400m-patch14-384\": \"google/siglip2-so400m-patch14-384\", \n    \"jina-clip-v2\": \"jinaai/jina-clip-v2\",\n    \"marqo-fashionSigLIP\": \"Marqo/marqo-fashionSigLIP\"\n}\n\ndef classify_with_model(image_path, labels, model_name=\"siglip-base-patch16-224\"):\n    \"\"\"Classify an image using a specified model\"\"\"\n    # Load model\n    model_path = MODELS[model_name]\n    classifier = pipeline(\n        model=model_path,\n        task=\"zero-shot-image-classification\",\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n    )\n    \n    # Load and classify image\n    image = Image.open(image_path)\n    results = classifier(image, candidate_labels=labels)\n    \n    # Display results\n    plt.figure(figsize=(10, 6))\n    labels = [result[\"label\"] for result in results]\n    scores = [result[\"score\"] for result in results]\n    \n    # Sort by score\n    sorted_indices = np.argsort(scores)[::-1]\n    sorted_labels = [labels[i] for i in sorted_indices]\n    sorted_scores = [scores[i] for i in sorted_indices]\n    \n    # Plot bar chart\n    bars = plt.barh(range(len(sorted_labels)), sorted_scores)\n    plt.yticks(range(len(sorted_labels)), sorted_labels)\n    plt.xlabel('Confidence Score')\n    plt.title(f'Classification Results with {model_name}')\n    \n    # Add percentage labels\n    for i, bar in enumerate(bars):\n        plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n                f'{sorted_scores[i]:.2%}', va='center')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return results\n\n# Example usage\nclassify_with_model(\n    \"fashion_item.jpg\",\n    [\"casual wear\", \"formal attire\", \"sportswear\", \"business casual\", \"vintage style\"],\n    \"siglip2-so400m-patch14-384\"\n)"
  },
  {
    "objectID": "posts/2025-03-22-zero-shot-image-classification/index.html#conclusions",
    "href": "posts/2025-03-22-zero-shot-image-classification/index.html#conclusions",
    "title": "Zero-Shot Image Classification with CLIP Models",
    "section": "Conclusions",
    "text": "Conclusions\nZero-shot classification is particularly valuable in domains like fashion where:\n\nCategories evolve quickly: New styles and trends emerge constantly\nAttribute-based classification: Items can be classified along multiple dimensions (style, occasion, material, etc.)\nSpecialized vocabulary: Fashion has domain-specific terminology that traditional classifiers struggle with\n\nZero-shot image classification with foundation models like CLIP and SigLIP models is useful for a range of applications and can be used to annotate unlabeled images. The reason these models are referred to as “foundation” models is because their general-purpose nature makes them applicable to a wide range of tasks, and they can be fine-tuned for specific applications."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI for the Second-Hand Fashion Industry",
    "section": "",
    "text": "The fashion industry contributes up to 10% of global greenhouse gas emissions. While the second-hand fashion sector offers a sustainable alternative, its operations remain largely manual. Our projects aim to drive this industry towards automation through a large annotated open dataset and AI models.\n\nView Dataset Read Updates"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "AI for the Second-Hand Fashion Industry",
    "section": "Recent Updates",
    "text": "Recent Updates\n\n\n\n\n\n\n\n\n\n\n    \n        Roadmap to full automation in sorting in the second-hand industry\n        2025-04-08\n        An exploration of the path to automation in the second-hand fashion industry, examining the steps needed for successful AI integration from initial assessment to full implementation.\n    \n    \n    \n        Zero-Shot Image Classification with CLIP Models\n        2025-03-22\n        Explore how CLIP and SigLIP models enable zero-shot image classification and their unique advantages over traditional image classifiers.\n    \n    \n    \n        Exploring the Second-Hand Fashion Dataset\n        2025-02-06\n        Deep dive into a 30,000+ image dataset for second-hand clothing classification."
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "AI for the Second-Hand Fashion Industry",
    "section": "Project Overview",
    "text": "Project Overview\nThe dataset for this project was created by a collaboration between Wargön Innovation AB, Myrorna AB and RISE Research Institutes of Sweden AB.\n(2025-02-06: The following section images are placeholders at the moment and will direct to model pages soon. In the meantime, check our demo from more than a year ago: fashion-demo)\n\n\n\n\n\nAutomated sorting process through Attribute Recognition\n\n\n\n\n\n\n\nAI-powered damage detection in clothing"
  },
  {
    "objectID": "index.html#stats",
    "href": "index.html#stats",
    "title": "AI for the Second-Hand Fashion Industry",
    "section": "Dataset Statistics",
    "text": "Dataset Statistics\n\n\n\nLoading dataset statistics…"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "AI for the Second-Hand Fashion Industry",
    "section": "Contact",
    "text": "Contact\nThis page was created by Farrukh Nauman (farrukh.nauman@inertialrange.com) and is not affiliated with any of the organizations mentioned above. I used to work at RISE until August 2025.\nFeel free to reach out to me:\n\nName: Farrukh Nauman\nEmail: farrukh.nauman@inertialrange.com\nWebsite: fnauman.github.io\nLinkedIn: Connect with me\nTwitter: Follow me"
  },
  {
    "objectID": "posts/2025-04-08-roadmap-to-full-automation/index.html",
    "href": "posts/2025-04-08-roadmap-to-full-automation/index.html",
    "title": "Roadmap to full automation in sorting in the second-hand industry",
    "section": "",
    "text": "Achieving full automation in any industry is rarely an overnight transformation. Instead, it requires a methodical, multi-stage process that often spans several years of dedicated effort. This is particularly true for the second-hand fashion industry, where unique challenges must be addressed before meaningful automation can take root."
  },
  {
    "objectID": "posts/2025-04-08-roadmap-to-full-automation/index.html#introduction",
    "href": "posts/2025-04-08-roadmap-to-full-automation/index.html#introduction",
    "title": "Roadmap to full automation in sorting in the second-hand industry",
    "section": "",
    "text": "Achieving full automation in any industry is rarely an overnight transformation. Instead, it requires a methodical, multi-stage process that often spans several years of dedicated effort. This is particularly true for the second-hand fashion industry, where unique challenges must be addressed before meaningful automation can take root."
  },
  {
    "objectID": "posts/2025-04-08-roadmap-to-full-automation/index.html#the-automation-journey-a-multi-stage-process",
    "href": "posts/2025-04-08-roadmap-to-full-automation/index.html#the-automation-journey-a-multi-stage-process",
    "title": "Roadmap to full automation in sorting in the second-hand industry",
    "section": "The Automation Journey: A Multi-Stage Process",
    "text": "The Automation Journey: A Multi-Stage Process\nThe road to automation typically involves:\n\nAssessment of the current situation - Understanding workflows, bottlenecks, and expertise requirements.\nDigitalization - Converting critical information into digital formats.\nDomain expertise capture - Finding systematic ways to record and transfer specialized knowledge.\nAI implementation - Deploying appropriate AI solutions where they add value.\n\nThis methodical approach has proven successful across various industries, but each sector faces unique implementation challenges."
  },
  {
    "objectID": "posts/2025-04-08-roadmap-to-full-automation/index.html#the-amazon-model-a-case-study-in-successful-automation",
    "href": "posts/2025-04-08-roadmap-to-full-automation/index.html#the-amazon-model-a-case-study-in-successful-automation",
    "title": "Roadmap to full automation in sorting in the second-hand industry",
    "section": "The Amazon Model: A Case Study in Successful Automation",
    "text": "The Amazon Model: A Case Study in Successful Automation\nConsider the logistics revolution at Amazon. Today, many Amazon facilities operate with near-complete automation - robots navigating warehouses, drones delivering packages, and AI systems orchestrating the entire process.\nThis level of automation wasn’t achieved overnight. Amazon’s journey began with:\n\nStandardizing packaging specifications.\nCreating comprehensive digital records of all shipments and inventory.\nBuilding AI models that could determine package locations and optimal handling methods.\nGradually integrating robotics and autonomous systems.\n\nThe result is one of the most sophisticated automated logistics operations in the world, but it required years of systematic development and investment."
  },
  {
    "objectID": "posts/2025-04-08-roadmap-to-full-automation/index.html#the-current-state-of-sorting-in-second-hand-fashion",
    "href": "posts/2025-04-08-roadmap-to-full-automation/index.html#the-current-state-of-sorting-in-second-hand-fashion",
    "title": "Roadmap to full automation in sorting in the second-hand industry",
    "section": "The Current State of Sorting in Second-Hand Fashion",
    "text": "The Current State of Sorting in Second-Hand Fashion\nIn stark contrast to Amazon’s advanced automation, the second-hand fashion sorting industry faces a fundamental obstacle: near-zero digitalization. Many of the world’s largest sorting facilities operate with:\n\nNo digital records of incoming or outgoing items\nMinimal standardization of processes\nHigh worker turnover with limited knowledge transfer\n\nThis creates a significant gap between the current state and the automation potential of the industry. Through our dataset, we are working to bridge this gap by encapsulating both the distribution of incoming items and the expertise of experienced sorters. This is just one of the many steps required to achieve full automation in the second-hand fashion industry."
  },
  {
    "objectID": "posts/2025-04-08-roadmap-to-full-automation/index.html#first-steps-toward-automation-in-second-hand-sorting",
    "href": "posts/2025-04-08-roadmap-to-full-automation/index.html#first-steps-toward-automation-in-second-hand-sorting",
    "title": "Roadmap to full automation in sorting in the second-hand industry",
    "section": "First Steps Toward Automation in Second-Hand Sorting",
    "text": "First Steps Toward Automation in Second-Hand Sorting\nTo begin the automation journey in second-hand fashion sorting, several foundational steps are essential:\n\nSystematic data recording - Implementing systems to track what sorting facilities receive, how items flow through the sorting process, and what ultimately leaves the facility.\nProcess standardization - Establishing consistent workflows and categorization systems across operations.\nMulti-location data collection - Gathering information from various sorting facilities to understand regional variations and biases in sorting practices.\nKnowledge capture - Developing methods to systematically document and transfer the expertise of experienced sorters.\n\nOnly after establishing these digital foundations can the industry begin meaningful implementation of AI and automation technologies that will transform operations.\nBy following this roadmap, the second-hand fashion industry can begin its journey toward the efficiency, consistency, and scalability that automation offers, while preserving the critical domain expertise that makes quality sorting possible."
  },
  {
    "objectID": "posts/2025-01-30-future-directions/index.html",
    "href": "posts/2025-01-30-future-directions/index.html",
    "title": "Future directions",
    "section": "",
    "text": "This project has been a success: we managed to create a one-of-a-kind dataset of second-hand fashion items that should have lasting value. Below, I list some challenges we encountered and some future directions for projects in this domain."
  },
  {
    "objectID": "posts/2025-01-30-future-directions/index.html#challenges",
    "href": "posts/2025-01-30-future-directions/index.html#challenges",
    "title": "Future directions",
    "section": "Challenges",
    "text": "Challenges\nData quality issues:\n\nThe dataset currently only includes image-level annotations, lacking bounding boxes or segmentation masks. This limits its applicability for tasks like detailed damage detection, which requires precisely locating damaged areas within the garment image.\nWe encountered issues with annotation quality and consistency. A significant portion of annotations were inaccurate leading to substantial manual effort in cleaning and rejecting unusable data. This increased the project’s workload and timeline.\nData collection was sometimes hampered by suboptimal image quality. Lower resolution images and inconsistent lighting conditions made annotation more challenging and potentially impacted the accuracy of AI models. Future data collection should prioritize high-resolution images under controlled lighting.\nThe lack of standardized annotation guidelines in the second-hand fashion industry posed a challenge. The absence of a common vocabulary for describing garment attributes makes it difficult to leverage pre-trained AI models and compare datasets. Developing industry-wide annotation standards would benefit research and development. Digital Product Passports (DPPs) could serve as a starting point.\n\nOrganizational challenges:\n\nEnsuring a baseline level of technical capacity across partner organizations is crucial. We experienced technical issues with equipment (e.g., camera setups, laptops) where some partner organizations lacked the expertise to resolve them independently. For future projects, upfront technical training and support for all partners is recommended.\nThe fragmented and underfunded nature of the second-hand fashion industry presents financing challenges. This makes it difficult for organizations and initiatives to secure resources for developing and scaling innovative AI projects. Dedicated funding and investment in this sector are essential.\nA gap in understanding AI and automation among some managers and representatives hindered project implementation. This lack of familiarity sometimes led to unrealistic expectations or difficulties integrating AI workflows. Investing in education and communication to bridge this knowledge gap is vital for successful AI adoption."
  },
  {
    "objectID": "posts/2025-01-30-future-directions/index.html#future-directions",
    "href": "posts/2025-01-30-future-directions/index.html#future-directions",
    "title": "Future directions",
    "section": "Future directions",
    "text": "Future directions\nA few general recommendations:\n\nPrioritize specialized AI applications: Focus on identifying and developing AI tools for specific, high-impact niche areas within the second-hand fashion sector, such as damage detection, automated quality assessment, or personalized recommendation systems.\nEmbrace Adaptability: The AI landscape is constantly changing. Given the typical 6-12 month cycles for securing funding and launching research projects (especially for large EU and national grants), it’s crucial to build adaptability into project design and be prepared to adjust plans as needed.\nLeverage Existing Technology and Expertise: Avoid reinventing the wheel. Prioritize the transfer and adaptation of existing AI technologies and methodologies. Actively seek technical expertise through hiring specialists, collaboration, or open-source resources.\nInvest in High-Quality Data: Building upon our challenges, future projects must prioritize high-resolution images under controlled conditions and robust, consistent annotations. This upfront investment in data quality will improve model performance and reduce data cleaning efforts.\n\nExample: Recycling/Material detection\nThe grand challenge in recycling is accurately identifying garment materials, especially for multi-layered garments and certain colors.\n\nFocus on Raw Sensor Data: To streamline recycling, access to raw sensor data is crucial. This includes data from NIR spectrometers, hyperspectral cameras, or other material sensing technologies for accurate material identification.\nAddress Recycling vs. Reuse: Distinguishing between garments for recycling and reuse is a challenge. This requires demand modeling in recycling and retail to assess reuse potential and recycling viability, considering garment condition, material, and market demand.\nDevelop Garment Lifecycle Tracking Systems: Implementing systems to track garments from production to end-of-life is crucial for optimizing resource utilization. This data can inform decisions on recycling, repair, or resale, maximizing lifespan and minimizing waste. Technologies like RFID tags or digital product passports could play a key role.\nPromote Innovative Recycling Technologies: Encourage research into novel recycling technologies for a wider range of textile materials. Advancements in material detection are a key enabler for these methods, paving the way for a circular fashion economy.\n\nExample: Localized Trends\n\nAddress Localized Fashion Trends: Highlight regional variations in fashion preferences. Effectively matching supply with local demand is a major challenge, leading to mismatches and missed opportunities.\nSource Data from First-Hand Retailers: To capture current trends, consider data from first-hand retailers. This provides more timely and representative information on consumer preferences than second-hand market data, which may reflect older trends.\nDevelop a Comprehensive Dataset: This dataset should include features capturing nuances of local trends, such as demographics, local events, weather patterns, and social media trends related to fashion in specific regions.\nEmbrace End-to-End Digitalization: To enable AI models to adapt to changing trends, a fully digital ecosystem is essential. This includes digitalizing inventory management, sales data, customer interactions, and supply chain processes for AI-driven trend analysis and decision-making."
  }
]