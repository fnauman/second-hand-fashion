[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Zero-Shot Image Classification with CLIP Models\n\n\n\ndeep-learning\n\n\ncomputer-vision\n\n\nnatural-language-processing\n\n\n\nExplore how CLIP and SigLIP models enable zero-shot image classification and their unique advantages over traditional image classifiers.\n\n\n\nFarrukh Nauman\n\n\nMar 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Second-Hand Fashion Dataset\n\n\n\ndata\n\n\nfashion\n\n\nsustainability\n\n\nmachine-learning\n\n\ncomputer-vision\n\n\n\nDeep dive into a 30,000+ image dataset for second-hand clothing classification.\n\n\n\nFarrukh Nauman\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFuture directions\n\n\n\nAI\n\n\nfashion-tech\n\n\nsustainability\n\n\n\nInsights into data quality challenges, organizational hurdles, and future opportunities in AI for second-hand fashion, including specialized applications in recycling and…\n\n\n\nFarrukh Nauman\n\n\nJan 30, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ignore_folder/gradio_migration_guide_4.html",
    "href": "ignore_folder/gradio_migration_guide_4.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nGradio 4.0 is a new major version, and includes breaking changes from 3.x. Here’s a list of all the breaking changes, along with migration steps where appropriate. You can also find this information in our changelog: https://www.gradio.app/changelog\nBreaking changes related to components:\n\nRemoves **kwarg from every component, meaning that components cannot accept arbitrary (unused) parameters. Previously, warnings would be thrown.\nRemoves deprecated parameters. For example, plain is no longer an alias for secondary for the variant argument in the gr.Button class\nRemoves the deprecated Carousel class and StatusTracker class and Box layout class\nRemoves the deprecated Variable alias for State\nRemoves the deprecated .style() methods from component classes\nRemoves the deprecated .update() method from component classes. Instead, you can now just return an instance of a component from your function. I.e. return gr.Textbox(lines=4) instead of gr.Textbox.update(lines=4)\nRemoves get_interpretation_neighbors() and get_interpretation_scores() from component classes\nRemoves deprecation.py – this was designed for internal usage so unlikely to break gradio apps\nMoves save to cache methods from component methods to standalone functions in processing_utils\nRenames source param in gr.Audio and gr.Video to sources\nRemoves show_edit_button param from `gr.Audio``\n\nOther changes related to the gradio library:\n\nRemoves the deprecated status_tracker parameter from events\nRemoves the deprecated HuggingFaceDatasetJSONSaver class\nNow Blocks.load() can only be use an is instance method to attach an event that runs when the page loads. To use the class method, use gr.load() instead\nSimilarly, Interface.load() has been removed\nIf you are runnin Gradio 4.x, you can not gr.load a Space that is running Gradio 3.x. However, you can still use the client libraries (see changes to the client libraries below).\nRemoves deprecated parameters, such as enable_queue from launch()\nMany of the positional arguments in launch() are now keyword only, and show_tips has been removed\nChanges the format of flagged data to json instead of filepath for media and chatbot\nRemoves gr.Series and gr.Parallel\nAll API endpoints are named by deafult. If api_name=None, the api name is the name of the python function.\n\nChanges related to the Client libraries:\n\nWhen using the gradio Client libraries in 3.x with any component that returned JSON data (including gr.Chatbot, gr.Label, and gr.JSON), the data would get saved to a file and the filepath would be returned. Similarly, you would have to pass input JSON as a filepath. Now, the JSON data is passed and returned directly, making it easier to work with these components using the clients.\n\nMigrating to Gradio 4.0\nHere are some concrete tips to help migrate to Gradio 4.0:\n\nUsing allowed_paths\n\nSince the working directory is now not served by default, if you reference local files within your CSS or in a gr.HTML component using the /file= route, you will need to explicitly allow access to those files (or their parent directories) using the allowed_paths parameter in launch()\nFor example, if your code looks like this:\nimport gradio as gr\nwith gr.Blocks() as demo: gr.HTML(“”)\ndemo.launch() In order for the HTML component to be able to serve image.png, you will need to add image.png in allowed_paths like this:\nimport gradio as gr\nwith gr.Blocks() as demo: gr.HTML(“”)\ndemo.launch(allowed_paths=[“image.png”]) or if you want to expose all files in your working directory as was the case in Gradio 3.x (not recommended if you plan to share your app with others), you could do:\nimport gradio as gr\nwith gr.Blocks() as demo: gr.HTML(“”)\ndemo.launch(allowed_paths=[“.”]) * Using concurrency_limit instead of concurrency_count\nPreviously, in Gradio 3.x, there was a single global concurrency_count parameter that controlled how many threads could execute tasks from the queue simultaneously. By default concurrency_count was 1, which meant that only a single event could be executed at a time (to avoid OOM errors when working with prediction functions that utilized a large amount of memory or GPU usage). You could bypass the queue by setting queue=False.\nIn Gradio 4.0, the concurrency_count parameter has been removed. You can still control the number of total threads by using the max_threads parameter. The default value of this parameter is 40, but you don’t have worry (as much) about OOM errors, because even though there are 40 threads, we use a single-worker-single-event model, which means each worker thread only executes a specific function. So effectively, each function has its own “concurrency count” of 1. If you’d like to change this behavior, you can do so by setting a parameter concurrency_limit, which is now a parameter of each event, not a global parameter. By default this is 1 for each event, but you can set it to a higher value, or to None if you’d like to allow an arbitrary number of executions of this event simultaneously. Events can also be grouped together using the concurrency_id parameter so that they share the same limit, and by default, events that call the same function share the same concurrency_id.\nTo summarize migration:\n\nFor events that execute quickly or don’t use much CPU or GPU resources, you should set concurrency_limit=None in Gradio 4.0. (Previously you would set queue=False.)\nFor events that take significant resources (like the prediction function of your machine learning model), and you only want 1 execution of this function at a time, you don’t have to set any parameters.\nFor events that take significant resources (like the prediction function of your machine learning model), and you only want X executions of this function at a time, you should set concurrency_limit=X parameter in the event trigger.(Previously you would set a global concurrency_count=X.)"
  },
  {
    "objectID": "posts/2025-02-06-data-exploration/index.html",
    "href": "posts/2025-02-06-data-exploration/index.html",
    "title": "Exploring the Second-Hand Fashion Dataset",
    "section": "",
    "text": "In this post, we’ll explore our one of a kind dataset that is focused on second-hand clothing classification."
  },
  {
    "objectID": "posts/2025-02-06-data-exploration/index.html#introduction",
    "href": "posts/2025-02-06-data-exploration/index.html#introduction",
    "title": "Exploring the Second-Hand Fashion Dataset",
    "section": "",
    "text": "In this post, we’ll explore our one of a kind dataset that is focused on second-hand clothing classification."
  },
  {
    "objectID": "posts/2025-02-06-data-exploration/index.html#dataset-overview",
    "href": "posts/2025-02-06-data-exploration/index.html#dataset-overview",
    "title": "Exploring the Second-Hand Fashion Dataset",
    "section": "Dataset Overview",
    "text": "Dataset Overview\nThe dataset contains 31,638 clothing items and was created as part of the Vinnova funded project “AI for resource-efficient circular fashion”. It’s a collaboration between RISE Research Institutes of Sweden AB, Wargön Innovation AB, and Myrorna AB, with additional support from the EU project CISUTAC.\nThe main purpose of this dataset is to help classify garments into various categories to determine their future use:\n\nReuse\nExport (reuse outside Sweden)\nRecycling\nRepair\nRemake\nThermal waste"
  },
  {
    "objectID": "posts/2025-02-06-data-exploration/index.html#interactive-dataset-explorer",
    "href": "posts/2025-02-06-data-exploration/index.html#interactive-dataset-explorer",
    "title": "Exploring the Second-Hand Fashion Dataset",
    "section": "Interactive Dataset Explorer",
    "text": "Interactive Dataset Explorer\nThe following is a version of the dataset that only uses the front imageYou can explore the dataset directly through the Hugging Face dataset viewer:\n\n\nYou can try simple search queries like “Shirt” or use the DuckDB interactive SQL editor to explore the dataset: SELECT * FROM train WHERE type = 'Shirt' AND condition = 4; for example will return all “Shirt” items that have a condition of 4 (i.e. “very good”). You might need to login to your huggingface account to use SQL queries. In my experience, the SQL queries run slowly."
  },
  {
    "objectID": "posts/2025-02-06-data-exploration/index.html#loading-the-dataset",
    "href": "posts/2025-02-06-data-exploration/index.html#loading-the-dataset",
    "title": "Exploring the Second-Hand Fashion Dataset",
    "section": "Loading the Dataset",
    "text": "Loading the Dataset\nTo work with this dataset programmatically, you can use the Hugging Face datasets library. Here’s how to load and explore the data:\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"fnauman/fashion-second-hand-front-only-rgb\")\n\n# Access the training split\ntrain_dataset = dataset[\"train\"]\n\n# Print basic information\nprint(f\"Dataset size: {len(train_dataset)} images\") # 28248\nprint(f\"Features: {train_dataset.features}\") # 19\n\n# Access an example\nexample = train_dataset[0]\n\nimage = example[\"image\"]\n# # Display the image - notebook\n# from IPython.display import display\n# display(example[\"image\"])\n\nprint(f\"Brand: {example['brand']}, Category: {example['category']}\")\n# Output: Brand: Soc (stadium), Category: Ladies\nThe image is:"
  },
  {
    "objectID": "posts/2025-02-06-data-exploration/index.html#dataset-features",
    "href": "posts/2025-02-06-data-exploration/index.html#dataset-features",
    "title": "Exploring the Second-Hand Fashion Dataset",
    "section": "Dataset Features",
    "text": "Dataset Features\nThe dataset provides RGB images of clothing items taken from the front view. Each image is labeled with 18 distinct attributes that can be used for:\n\nTraining computer vision models for clothing classification: type, category, etc.\nDetecting condition and damage.\nBuilding image-based and text-based search systems."
  },
  {
    "objectID": "posts/2025-02-06-data-exploration/index.html#license-and-attribution",
    "href": "posts/2025-02-06-data-exploration/index.html#license-and-attribution",
    "title": "Exploring the Second-Hand Fashion Dataset",
    "section": "License and Attribution",
    "text": "License and Attribution\nThe dataset is released under CC-BY 4.0 license. When using this dataset, make sure to properly attribute the zenodo version:\n\nNauman, F. (2024). Clothing Dataset for Second-Hand Fashion (Version 3) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.13788681"
  },
  {
    "objectID": "posts/2025-02-06-data-exploration/index.html#next-steps",
    "href": "posts/2025-02-06-data-exploration/index.html#next-steps",
    "title": "Exploring the Second-Hand Fashion Dataset",
    "section": "Next Steps",
    "text": "Next Steps\nIn future posts, we’ll explore:\n\nBuilding a classification model using this dataset\nCreating a simple web application for clothing classification\n\nStay tuned for more insights into sustainable fashion and AI!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI for the Second-Hand Fashion Industry",
    "section": "",
    "text": "The fashion industry contributes up to 10% of global greenhouse gas emissions. While the second-hand fashion sector offers a sustainable alternative, its operations remain largely manual. Our projects aim to drive this industry towards automation through a large annotated open dataset and AI models.\n\nView Dataset Read Updates"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "AI for the Second-Hand Fashion Industry",
    "section": "Recent Updates",
    "text": "Recent Updates\n\n\n\n\n\n\n\n\n\n\n    \n        Zero-Shot Image Classification with CLIP Models\n        2025-03-22\n        Explore how CLIP and SigLIP models enable zero-shot image classification and their unique advantages over traditional image classifiers.\n    \n    \n    \n        Exploring the Second-Hand Fashion Dataset\n        2025-02-06\n        Deep dive into a 30,000+ image dataset for second-hand clothing classification.\n    \n    \n    \n        Future directions\n        2025-01-30\n        Insights into data quality challenges, organizational hurdles, and future opportunities in AI for second-hand fashion, including specialized applications in recycling and local trend analysis."
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "AI for the Second-Hand Fashion Industry",
    "section": "Project Overview",
    "text": "Project Overview\nThe dataset for this project was created by a collaboration between Wargön Innovation AB, Myrorna AB and RISE Research Institutes of Sweden AB.\n(2025-02-06: The following section images are placeholders at the moment and will direct to model pages soon. In the meantime, check our demo from more than a year ago: fashion-demo)\n\n\n\n\n\nAutomated sorting process through Attribute Recognition\n\n\n\n\n\n\n\nAI-powered damage detection in clothing"
  },
  {
    "objectID": "index.html#stats",
    "href": "index.html#stats",
    "title": "AI for the Second-Hand Fashion Industry",
    "section": "Dataset Statistics",
    "text": "Dataset Statistics\n\n\n\nLoading dataset statistics…"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "AI for the Second-Hand Fashion Industry",
    "section": "Contact",
    "text": "Contact\nFeel free to reach out to me:\n\nName: Farrukh Nauman\nEmail: farrukh.nauman@ri.se\nWebsite: fnauman.github.io\nLinkedIn: Connect with me\nTwitter: Follow me"
  },
  {
    "objectID": "posts/2025-01-30-future-directions/index.html",
    "href": "posts/2025-01-30-future-directions/index.html",
    "title": "Future directions",
    "section": "",
    "text": "This project has been a success: we managed to create a one-of-a-kind dataset of second-hand fashion items that should have lasting value. Below, I list some challenges we encountered and some future directions for projects in this domain."
  },
  {
    "objectID": "posts/2025-01-30-future-directions/index.html#challenges",
    "href": "posts/2025-01-30-future-directions/index.html#challenges",
    "title": "Future directions",
    "section": "Challenges",
    "text": "Challenges\nData quality issues:\n\nThe dataset currently only includes image-level annotations, lacking bounding boxes or segmentation masks. This limits its applicability for tasks like detailed damage detection, which requires precisely locating damaged areas within the garment image.\nWe encountered issues with annotation quality and consistency. A significant portion of annotations were inaccurate leading to substantial manual effort in cleaning and rejecting unusable data. This increased the project’s workload and timeline.\nData collection was sometimes hampered by suboptimal image quality. Lower resolution images and inconsistent lighting conditions made annotation more challenging and potentially impacted the accuracy of AI models. Future data collection should prioritize high-resolution images under controlled lighting.\nThe lack of standardized annotation guidelines in the second-hand fashion industry posed a challenge. The absence of a common vocabulary for describing garment attributes makes it difficult to leverage pre-trained AI models and compare datasets. Developing industry-wide annotation standards would benefit research and development. Digital Product Passports (DPPs) could serve as a starting point.\n\nOrganizational challenges:\n\nEnsuring a baseline level of technical capacity across partner organizations is crucial. We experienced technical issues with equipment (e.g., camera setups, laptops) where some partner organizations lacked the expertise to resolve them independently. For future projects, upfront technical training and support for all partners is recommended.\nThe fragmented and underfunded nature of the second-hand fashion industry presents financing challenges. This makes it difficult for organizations and initiatives to secure resources for developing and scaling innovative AI projects. Dedicated funding and investment in this sector are essential.\nA gap in understanding AI and automation among some managers and representatives hindered project implementation. This lack of familiarity sometimes led to unrealistic expectations or difficulties integrating AI workflows. Investing in education and communication to bridge this knowledge gap is vital for successful AI adoption."
  },
  {
    "objectID": "posts/2025-01-30-future-directions/index.html#future-directions",
    "href": "posts/2025-01-30-future-directions/index.html#future-directions",
    "title": "Future directions",
    "section": "Future directions",
    "text": "Future directions\nA few general recommendations:\n\nPrioritize specialized AI applications: Focus on identifying and developing AI tools for specific, high-impact niche areas within the second-hand fashion sector, such as damage detection, automated quality assessment, or personalized recommendation systems.\nEmbrace Adaptability: The AI landscape is constantly changing. Given the typical 6-12 month cycles for securing funding and launching research projects (especially for large EU and national grants), it’s crucial to build adaptability into project design and be prepared to adjust plans as needed.\nLeverage Existing Technology and Expertise: Avoid reinventing the wheel. Prioritize the transfer and adaptation of existing AI technologies and methodologies. Actively seek technical expertise through hiring specialists, collaboration, or open-source resources.\nInvest in High-Quality Data: Building upon our challenges, future projects must prioritize high-resolution images under controlled conditions and robust, consistent annotations. This upfront investment in data quality will improve model performance and reduce data cleaning efforts.\n\nExample: Recycling/Material detection\nThe grand challenge in recycling is accurately identifying garment materials, especially for multi-layered garments and certain colors.\n\nFocus on Raw Sensor Data: To streamline recycling, access to raw sensor data is crucial. This includes data from NIR spectrometers, hyperspectral cameras, or other material sensing technologies for accurate material identification.\nAddress Recycling vs. Reuse: Distinguishing between garments for recycling and reuse is a challenge. This requires demand modeling in recycling and retail to assess reuse potential and recycling viability, considering garment condition, material, and market demand.\nDevelop Garment Lifecycle Tracking Systems: Implementing systems to track garments from production to end-of-life is crucial for optimizing resource utilization. This data can inform decisions on recycling, repair, or resale, maximizing lifespan and minimizing waste. Technologies like RFID tags or digital product passports could play a key role.\nPromote Innovative Recycling Technologies: Encourage research into novel recycling technologies for a wider range of textile materials. Advancements in material detection are a key enabler for these methods, paving the way for a circular fashion economy.\n\nExample: Localized Trends\n\nAddress Localized Fashion Trends: Highlight regional variations in fashion preferences. Effectively matching supply with local demand is a major challenge, leading to mismatches and missed opportunities.\nSource Data from First-Hand Retailers: To capture current trends, consider data from first-hand retailers. This provides more timely and representative information on consumer preferences than second-hand market data, which may reflect older trends.\nDevelop a Comprehensive Dataset: This dataset should include features capturing nuances of local trends, such as demographics, local events, weather patterns, and social media trends related to fashion in specific regions.\nEmbrace End-to-End Digitalization: To enable AI models to adapt to changing trends, a fully digital ecosystem is essential. This includes digitalizing inventory management, sales data, customer interactions, and supply chain processes for AI-driven trend analysis and decision-making."
  },
  {
    "objectID": "posts/2025-03-22-zero-shot-image-classification/index.html",
    "href": "posts/2025-03-22-zero-shot-image-classification/index.html",
    "title": "Zero-Shot Image Classification with CLIP Models",
    "section": "",
    "text": "Imagine you’re sorting through a huge pile of clothes for your online second-hand store. You’ve got everything from vintage dresses to modern t-shirts, and you need to quickly categorize them all. Traditional image classifiers are like robots trained to only recognize a specific set of clothing types they’ve seen before. If you suddenly have a “bohemian skirt” – something outside their training – they’d be stumped!\nBut what if you could use a smarter kind of AI? One that can understand the idea of a “bohemian skirt” just from the words, even if it’s never seen one exactly like it before? That’s the magic of zero-shot image classification, and it’s changing how we can use computers to understand images, especially in fields like fashion where trends and categories are always evolving.\nIn this post, we’ll explore how models called CLIP (Contrastive Language-Image Pre-training) and SigLIP (Sigmoid Loss Pre-training) make this “zero-shot” magic possible. They are vision-language models, meaning they understand both images and text, allowing them to classify images in incredibly flexible ways, without needing to be specifically trained on every single category beforehand. This is a game-changer for anyone dealing with visual categorization tasks, especially in dynamic domains like fashion."
  },
  {
    "objectID": "posts/2025-03-22-zero-shot-image-classification/index.html#introduction",
    "href": "posts/2025-03-22-zero-shot-image-classification/index.html#introduction",
    "title": "Zero-Shot Image Classification with CLIP Models",
    "section": "",
    "text": "Imagine you’re sorting through a huge pile of clothes for your online second-hand store. You’ve got everything from vintage dresses to modern t-shirts, and you need to quickly categorize them all. Traditional image classifiers are like robots trained to only recognize a specific set of clothing types they’ve seen before. If you suddenly have a “bohemian skirt” – something outside their training – they’d be stumped!\nBut what if you could use a smarter kind of AI? One that can understand the idea of a “bohemian skirt” just from the words, even if it’s never seen one exactly like it before? That’s the magic of zero-shot image classification, and it’s changing how we can use computers to understand images, especially in fields like fashion where trends and categories are always evolving.\nIn this post, we’ll explore how models called CLIP (Contrastive Language-Image Pre-training) and SigLIP (Sigmoid Loss Pre-training) make this “zero-shot” magic possible. They are vision-language models, meaning they understand both images and text, allowing them to classify images in incredibly flexible ways, without needing to be specifically trained on every single category beforehand. This is a game-changer for anyone dealing with visual categorization tasks, especially in dynamic domains like fashion."
  },
  {
    "objectID": "posts/2025-03-22-zero-shot-image-classification/index.html#what-makes-clip-and-siglip-special",
    "href": "posts/2025-03-22-zero-shot-image-classification/index.html#what-makes-clip-and-siglip-special",
    "title": "Zero-Shot Image Classification with CLIP Models",
    "section": "What Makes CLIP and SigLIP Special?",
    "text": "What Makes CLIP and SigLIP Special?\nCLIP and SigLIP are fundamentally different from traditional image classifiers like ResNet or EfficientNet (and even Vision Transformers (ViT) used solely for classification) in several important ways:\n\nJoint Vision-Language Training (Contrastive Learning): This is the core difference.\n\nTraditional Classifiers (ResNet, ViT): These models are trained on images labeled with a fixed set of categories. They learn to map image features to these specific labels. They only learn visual features. The output layer has a fixed number of neurons, one for each class.\nCLIP/SigLIP: These models are trained on image-text pairs. The training objective is contrastive. Given a batch of images and texts, the model learns to:\n\nMaximize the similarity between the embeddings of an image and its correct text description.\nMinimize the similarity between the embeddings of an image and incorrect text descriptions. This is often done using a contrastive loss function (e.g., InfoNCE). SigLIP uses a sigmoid cross-entropy loss instead of the softmax-based contrastive loss in CLIP, which has been shown to improve performance. This creates a joint embedding space where images and text representing similar concepts are close together, even if the model hasn’t seen that exact combination during training.\n\n\nMassive Datasets: These models are pre-trained on 100M-1B+ image-text pairs collected from the internet. This “web” scale pretraining data ensures that the models have seen a wide range of visual and textual concepts, making them more robust and versatile.\nNo Predefined Classes (During Inference):\n\nTraditional Classifiers: Require a predefined set of output classes. You can’t ask them to classify something outside of that set without retraining or fine-tuning.\nCLIP/SigLIP: Can classify images against any arbitrary text description at inference time. The model compares the image embedding to the embeddings of the provided text descriptions. This flexibility is the essence of zero-shot learning. One potential limitation is that certain unique concepts or names like specific new brands or products may not be recognized."
  },
  {
    "objectID": "posts/2025-03-22-zero-shot-image-classification/index.html#how-zero-shot-classification-works",
    "href": "posts/2025-03-22-zero-shot-image-classification/index.html#how-zero-shot-classification-works",
    "title": "Zero-Shot Image Classification with CLIP Models",
    "section": "How Zero-Shot Classification Works",
    "text": "How Zero-Shot Classification Works\nThe core mechanism behind zero-shot classification with CLIP-like models is elegantly simple:\n\nEncoding: The model uses separate encoders (typically Transformers) for images and text. The image is passed through the image encoder, and each potential text label is passed through the text encoder. This produces an image embedding and a set of text embeddings, all within the same shared embedding space.\nSimilarity Calculation: The model then computes the similarity between the image embedding and each text embedding. Cosine similarity is commonly used.\nPrediction: The text label with the highest similarity score is the predicted class.\n\nThis approach allows you to classify images using any text descriptions you provide on the fly, without retraining the model."
  },
  {
    "objectID": "posts/2025-03-22-zero-shot-image-classification/index.html#zero-shot-classification-with-transformers",
    "href": "posts/2025-03-22-zero-shot-image-classification/index.html#zero-shot-classification-with-transformers",
    "title": "Zero-Shot Image Classification with CLIP Models",
    "section": "Zero-Shot Classification with Transformers",
    "text": "Zero-Shot Classification with Transformers\nLet’s implement a simple zero-shot image classifier using the Hugging Face transformers library:\n\n\nCode\nfrom transformers import pipeline\nfrom PIL import Image\n\n# Load a pre-trained CLIP/SigLIP model\nclassifier = pipeline(\n    \"zero-shot-image-classification\",\n    model=\"google/siglip-base-patch16-224\",\n)\n\n# Load an image\nimage = Image.open(\"fashion_item.jpg\")\n\n# Define classification labels\ncandidate_labels = [\n    \"t-shirt\", \n    \"dress\", \n    \"jacket\", \n    \"jeans\"\n]\n\n# Perform zero-shot classification\nresults = classifier(\n    image, \n    candidate_labels=candidate_labels\n)\n\n# Print results\nfor result in results:\n    print(f\"{result['label']}: {result['score']:.2%}\")\n\n# Output:\n# t-shirt: 76.32%\n# dress: 15.87%\n# jacket: 4.51%\n# jeans: 2.15%\n\n\nThe model assigns a confidence score to each label without ever being explicitly trained on fashion categories."
  },
  {
    "objectID": "posts/2025-03-22-zero-shot-image-classification/index.html#advanced-example-custom-model-selection",
    "href": "posts/2025-03-22-zero-shot-image-classification/index.html#advanced-example-custom-model-selection",
    "title": "Zero-Shot Image Classification with CLIP Models",
    "section": "Advanced Example: Custom Model Selection",
    "text": "Advanced Example: Custom Model Selection\nYou can choose from several state-of-the-art vision-language models for zero-shot classification. Here’s a more advanced example with model selection:\n\n\nCode\nimport torch\nfrom transformers import pipeline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Available models\nMODELS = {\n    \"siglip-base-patch16-224\": \"google/siglip-base-patch16-224\", \n    \"siglip2-base-patch16-224\": \"google/siglip2-base-patch16-224\",\n    \"siglip2-so400m-patch14-384\": \"google/siglip2-so400m-patch14-384\", \n    \"jina-clip-v2\": \"jinaai/jina-clip-v2\",\n    \"marqo-fashionSigLIP\": \"Marqo/marqo-fashionSigLIP\"\n}\n\ndef classify_with_model(image_path, labels, model_name=\"siglip-base-patch16-224\"):\n    \"\"\"Classify an image using a specified model\"\"\"\n    # Load model\n    model_path = MODELS[model_name]\n    classifier = pipeline(\n        model=model_path,\n        task=\"zero-shot-image-classification\",\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n    )\n    \n    # Load and classify image\n    image = Image.open(image_path)\n    results = classifier(image, candidate_labels=labels)\n    \n    # Display results\n    plt.figure(figsize=(10, 6))\n    labels = [result[\"label\"] for result in results]\n    scores = [result[\"score\"] for result in results]\n    \n    # Sort by score\n    sorted_indices = np.argsort(scores)[::-1]\n    sorted_labels = [labels[i] for i in sorted_indices]\n    sorted_scores = [scores[i] for i in sorted_indices]\n    \n    # Plot bar chart\n    bars = plt.barh(range(len(sorted_labels)), sorted_scores)\n    plt.yticks(range(len(sorted_labels)), sorted_labels)\n    plt.xlabel('Confidence Score')\n    plt.title(f'Classification Results with {model_name}')\n    \n    # Add percentage labels\n    for i, bar in enumerate(bars):\n        plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n                f'{sorted_scores[i]:.2%}', va='center')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return results\n\n# Example usage\nclassify_with_model(\n    \"fashion_item.jpg\",\n    [\"casual wear\", \"formal attire\", \"sportswear\", \"business casual\", \"vintage style\"],\n    \"siglip2-so400m-patch14-384\"\n)"
  },
  {
    "objectID": "posts/2025-03-22-zero-shot-image-classification/index.html#conclusions",
    "href": "posts/2025-03-22-zero-shot-image-classification/index.html#conclusions",
    "title": "Zero-Shot Image Classification with CLIP Models",
    "section": "Conclusions",
    "text": "Conclusions\nZero-shot classification is particularly valuable in domains like fashion where:\n\nCategories evolve quickly: New styles and trends emerge constantly\nAttribute-based classification: Items can be classified along multiple dimensions (style, occasion, material, etc.)\nSpecialized vocabulary: Fashion has domain-specific terminology that traditional classifiers struggle with\n\nZero-shot image classification with foundation models like CLIP and SigLIP models is useful for a range of applications and can be used to annotate unlabeled images. The reason these models are referred to as “foundation” models is because their general-purpose nature makes them applicable to a wide range of tasks, and they can be fine-tuned for specific applications."
  },
  {
    "objectID": "ignore_folder/gradio_common_errors.html",
    "href": "ignore_folder/gradio_common_errors.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "ignore_folder/gradio_common_errors.html#gradio-4.0-migration-common-errors",
    "href": "ignore_folder/gradio_common_errors.html#gradio-4.0-migration-common-errors",
    "title": "",
    "section": "gradio 4.0 migration common errors",
    "text": "gradio 4.0 migration common errors\n\nstyle() method of components has been deprecated\n    btn_text_query = gr.Button(\"Retrieve Garments\").style(size=\"sm\")\n    text_query_gallery = gr.Gallery(show_label=False).style(rows=1, columns=3)\nshould be replaced with:\n    btn_text_query = gr.Button(\"Retrieve Garments\", size=\"sm\")\n    text_query_gallery = gr.Gallery(show_label=False, rows=1, columns=3)\n\n\nupdate() method of components has been deprecated\nYes, in Gradio 4.x the .update() method of components has been deprecated, now you can simply return the component itself\nYou’ll want to make sure the output Dropdown component is interactive so that you can click on it and look through the options. By default, Gradio sets output components to be non-interactive, but you can change that by passing in interactive=True\nThis is a working version of your app:\nimport gradio as gr\n\ncategories = ['Long-Short Equity', 'Long Government', 'Multisector Bond', 'Emerging Markets Bond', 'Corporate Bond', 'Intermediate Government', 'Inflation-Protected Bond', 'Intermediate-Term Bond', 'Muni National Long', 'Unknown', 'High Yield Muni', 'Long-Term Bond', 'Muni California Long', 'Muni National Interm', 'Nontraditional Bond', 'World Bond', 'Short Government', 'Muni National Short', 'Short-Term Bond', 'Preferred Stock', 'Ultrashort Bond', 'High Yield Bond', 'Muni New York Long', 'Emerging-Markets Local-Currency Bond', 'Miscellaneous Region', 'Bank Loan', 'Commodities Broad Basket', 'Japan Stock', 'World Allocation', 'Tactical Allocation', 'Large Value', 'Foreign Large Growth', 'Energy Limited Partnership', 'Foreign Small/Mid Blend', 'Foreign Large Value', 'Foreign Large Blend', 'Europe Stock', 'Allocation--50% to 70% Equity', 'Financial', 'Diversified Emerging Mkts', 'Industrials', 'Mid-Cap Blend', 'Large Growth', 'Communications', 'Diversified Pacific/Asia', 'Foreign Small/Mid Value', 'Convertibles', 'Small Value', 'Latin America Stock', 'Equity Energy', 'Natural Resources', 'Real Estate', 'Large Blend', 'Small Blend', 'Consumer Cyclical']\n\ndef update_symbols(category):\n    symbols =  ['FFIU', 'IGEB', 'VCIT', 'FCOR', 'SKOR', 'KORP', 'LQDI']\n    return gr.Dropdown(choices=symbols, interactive=True)\n\n# Create the Gradio interface\nwith gr.Blocks() as demo:\n    gr.Markdown(\"### Dropdown Tester\")\n    with gr.Row():\n        category_dropdown = gr.Dropdown(choices=categories, label=\"Category\")\n        symbol_dropdown = gr.Dropdown(label=\"Symbols\", choices=[])\n\n    # Update the symbols dropdown when the category changes\n    category_dropdown.change(update_symbols, inputs=[category_dropdown], outputs=[symbol_dropdown])\n\ndemo.launch()"
  }
]